{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrmitdLZDEY1geQge6oD/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arseniy-Polyakov/russian_sign_language_recognition/blob/main/Sign_Language_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --upgrade --quiet"
      ],
      "metadata": {
        "id": "jo9gU4dVg70z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "U-U6Qguejg5v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"marian-mt-translation\",\n",
        "    name=\"marian-mt-ru-rsl\"\n",
        ")"
      ],
      "metadata": {
        "id": "Kmwnw0I_ncDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\n",
        "        \"train\": \"train.csv\",\n",
        "        \"test\": \"test.csv\"\n",
        "    }\n",
        ")\n",
        "dataset"
      ],
      "metadata": {
        "id": "gApNrgguji8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78fbc1b-edb8-4acf-c943-f3ab26b0b95f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['russian', 'rsl'],\n",
              "        num_rows: 80\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['russian', 'rsl'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing(tokenizer):\n",
        "    def preprocess(batch):\n",
        "        model_inputs = tokenizer(\n",
        "            batch[\"russian\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(\n",
        "                batch[\"rsl\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=128\n",
        "            )\n",
        "        labels_ids = labels[\"input_ids\"]\n",
        "        labels_ids = [\n",
        "            [(tok if tok != tokenizer.pad_token_id else -100) for tok in sent]\n",
        "            for sent in labels_ids\n",
        "        ]\n",
        "\n",
        "        model_inputs[\"labels\"] = labels_ids\n",
        "        return model_inputs\n",
        "    return preprocess"
      ],
      "metadata": {
        "id": "64zVd_y_013B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = \"facebook/bart-base\"\n",
        "# model_path = \"google/mt5-small\"\n",
        "model_path = \"hf-internal-testing/tiny-random-MarianMTModel\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "# tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "NrXm-d5OvljO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = tokenizing(tokenizer)\n",
        "tokenized_dataset = dataset.map(preprocessing, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "AqEY-FtB1pYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"marian-mt\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1000,\n",
        "    learning_rate=5e-5,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"marian-mt-ru-rsl\",\n",
        "    eval_steps=500,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "c9rYLXDWj6h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_vf7Eg0SlgTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CUYj6fPMorSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "jNe0kyWD70VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_texts(texts, model, tokenizer, device=\"cpu\", max_length=128, num_beams=5):\n",
        "    model.to(device)\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    translations = tokenizer.batch_decode(\n",
        "        outputs,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return translations"
      ],
      "metadata": {
        "id": "h1WMwrZx8DZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"giga_chat_dataset.csv\")\n",
        "data_for_translation = df[\"russian\"].to_list()\n",
        "\n",
        "translations = translate_texts(\n",
        "    data_for_translation,\n",
        "    model,\n",
        "    tokenizer\n",
        ")\n",
        "translations"
      ],
      "metadata": {
        "id": "AGFLhlWC9cjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}